{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6fbf208",
   "metadata": {},
   "source": [
    "# Makemore: bigrams with neural-nets\n",
    "\n",
    "### [The spelled-out intro to language modeling: building makemore](https://www.youtube.com/watch?v=PaCmpygFfXo)\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d29e07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3beff9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('names.txt') as f:\n",
    "    names = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00781a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21b7cdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['.'] + list(string.ascii_lowercase)\n",
    "t_i = {t:i for i,t in enumerate(tokens)}\n",
    "i_t = {i:t for t,i in t_i.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e404b50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating training set\n",
    "\n",
    "inputs = []\n",
    "targets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da469c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in names[:1]:\n",
    "    word = ['.'] + list(word) + ['.']\n",
    "    for ch1, ch2 in zip(word,word[1:]):\n",
    "        r = t_i[ch1]\n",
    "        c = t_i[ch2]\n",
    "        inputs.append(r)\n",
    "        targets.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18e86d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  5, 13, 13,  1]), tensor([ 5, 13, 13,  1,  0]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.tensor(inputs)\n",
    "targets = torch.tensor(targets)\n",
    "inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd7af436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we one-hot encode these int tensors\n",
    "\n",
    "X_enc = F.one_hot(inputs, num_classes=len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a64da64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 27])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fbbd7223e20>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(X_enc.shape)\n",
    "plt.imshow(X_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ccda2d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_enc.dtype # we need float for inputs not int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8ee14d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_enc.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e51064ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5819],\n",
       "        [-0.7410],\n",
       "        [ 0.5692],\n",
       "        [ 0.5692],\n",
       "        [-0.4384]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.randn(size=(len(tokens),1))\n",
    "\"\"\"\n",
    "w: 27x1\n",
    "X: 5x27\n",
    "\n",
    "X@w: 5x1\n",
    "\"\"\"\n",
    "X_enc.float() @ w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5a4fe25c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's use 27 neurons\n",
    "# weights of each neuron are along the column\n",
    "\"\"\"\n",
    "w: 27x27\n",
    "X: 5x27\n",
    "\n",
    "X@w: 5x27\n",
    "\"\"\"\n",
    "\n",
    "w = torch.randn(size=(27,27))\n",
    "(X_enc.float() @ w).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b8fd54fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_enc.float()[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a6b5094e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6147, -1.2964, -0.1946,  1.0723, -0.7888,  0.3645, -0.8814,  0.8644,\n",
       "        -1.0817,  0.6416,  1.1896,  0.2607,  0.7135,  0.9022, -0.8449, -0.2512,\n",
       "        -0.3600, -1.3310, -0.4677,  0.9165,  1.3682, -0.9608,  0.5042,  0.5871,\n",
       "         0.3524, -0.7845, -0.4747])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w[:,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7bc73377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9022)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_enc.float()[3] * w[:,13]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d03b8a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0054, 0.0394, 0.0210, 0.0157, 0.0292, 0.0119, 0.0084, 0.0310, 0.0184,\n",
       "         0.0206, 0.0170, 0.0106, 0.0543, 0.0146, 0.0164, 0.0267, 0.0086, 0.0037,\n",
       "         0.0249, 0.0347, 0.0466, 0.1891, 0.0442, 0.0833, 0.0223, 0.0076, 0.1944],\n",
       "        [0.0279, 0.0219, 0.0066, 0.0120, 0.0332, 0.0170, 0.0133, 0.0075, 0.0054,\n",
       "         0.0113, 0.0139, 0.0128, 0.0121, 0.0057, 0.0979, 0.0323, 0.0427, 0.0846,\n",
       "         0.0137, 0.0132, 0.0071, 0.0201, 0.0287, 0.0035, 0.3129, 0.0294, 0.1136],\n",
       "        [0.0464, 0.0471, 0.0399, 0.0156, 0.0214, 0.0183, 0.0033, 0.0381, 0.1093,\n",
       "         0.0440, 0.0091, 0.0648, 0.0294, 0.0182, 0.0069, 0.0021, 0.1062, 0.0162,\n",
       "         0.0138, 0.0099, 0.0478, 0.0471, 0.0088, 0.0058, 0.1672, 0.0524, 0.0109],\n",
       "        [0.0464, 0.0471, 0.0399, 0.0156, 0.0214, 0.0183, 0.0033, 0.0381, 0.1093,\n",
       "         0.0440, 0.0091, 0.0648, 0.0294, 0.0182, 0.0069, 0.0021, 0.1062, 0.0162,\n",
       "         0.0138, 0.0099, 0.0478, 0.0471, 0.0088, 0.0058, 0.1672, 0.0524, 0.0109],\n",
       "        [0.0475, 0.0696, 0.0153, 0.0031, 0.0861, 0.0095, 0.0085, 0.0090, 0.0067,\n",
       "         0.1047, 0.0232, 0.0462, 0.0149, 0.0361, 0.0755, 0.0857, 0.0192, 0.0271,\n",
       "         0.0026, 0.0067, 0.0125, 0.0519, 0.0160, 0.0094, 0.1534, 0.0456, 0.0140]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from this multiplication we need log counts\n",
    "# we will exponentiate them\n",
    "# then probabilities will be the normalized log counts\n",
    "\n",
    "logits = (X_enc.float() @ w) # log counts\n",
    "counts = logits.exp() # equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4a9ed5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0b3909b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5),\n",
       " tensor([0.0054, 0.0394, 0.0210, 0.0157, 0.0292, 0.0119, 0.0084, 0.0310, 0.0184,\n",
       "         0.0206, 0.0170, 0.0106, 0.0543, 0.0146, 0.0164, 0.0267, 0.0086, 0.0037,\n",
       "         0.0249, 0.0347, 0.0466, 0.1891, 0.0442, 0.0833, 0.0223, 0.0076, 0.1944]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[0], probs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffca70c8",
   "metadata": {},
   "source": [
    "so what we need to do now is calculate weights and then backpropagate to minimize nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "53499c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  5, 13, 13,  1]), tensor([ 5, 13, 13,  1,  0]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c9da4f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2147483647\n",
    "gen = torch.Generator().manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ed391dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn((27,27),generator=gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "50780117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "\n",
    "x_enc = F.one_hot(inputs, num_classes=len(tokens)).float()\n",
    "logits = x_enc @ W\n",
    "\n",
    "# these next 2 lines is basically softmax\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4c6f6098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1e993926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "bigram: .e\tindices: 0,5\n",
      "neural network input: 0\n",
      "output probabilities: tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
      "        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
      "        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459])\n",
      "actual output [label]: 5\n",
      "predicted label: 0.01228625513613224\n",
      "log likelihood: -4.399273872375488\tnegative log likelihood: 4.399273872375488\n",
      "-------------------------\n",
      "-------------------------\n",
      "bigram: em\tindices: 5,13\n",
      "neural network input: 5\n",
      "output probabilities: tensor([0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n",
      "        0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n",
      "        0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472])\n",
      "actual output [label]: 13\n",
      "predicted label: 0.018050700426101685\n",
      "log likelihood: -4.014570713043213\tnegative log likelihood: 4.014570713043213\n",
      "-------------------------\n",
      "-------------------------\n",
      "bigram: mm\tindices: 13,13\n",
      "neural network input: 13\n",
      "output probabilities: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
      "actual output [label]: 13\n",
      "predicted label: 0.026691533625125885\n",
      "log likelihood: -3.623408794403076\tnegative log likelihood: 3.623408794403076\n",
      "-------------------------\n",
      "-------------------------\n",
      "bigram: ma\tindices: 13,1\n",
      "neural network input: 13\n",
      "output probabilities: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
      "actual output [label]: 1\n",
      "predicted label: 0.07367686182260513\n",
      "log likelihood: -2.6080665588378906\tnegative log likelihood: 2.6080665588378906\n",
      "-------------------------\n",
      "-------------------------\n",
      "bigram: a.\tindices: 1,0\n",
      "neural network input: 1\n",
      "output probabilities: tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
      "        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
      "        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091])\n",
      "actual output [label]: 0\n",
      "predicted label: 0.014977526850998402\n",
      "log likelihood: -4.201204299926758\tnegative log likelihood: 4.201204299926758\n",
      "-------------------------\n",
      "==============================\n",
      "average nll: 3.7693049907684326\n"
     ]
    }
   ],
   "source": [
    "nlls = torch.zeros(5)\n",
    "\n",
    "for i in range(5):\n",
    "    # i-th bigram\n",
    "    x = inputs[i].item()\n",
    "    y = targets[i].item()\n",
    "    print('-'*25)\n",
    "    print(f\"bigram: {i_t[x]}{i_t[y]}\\tindices: {x},{y}\")\n",
    "    print(f\"neural network input: {x}\")\n",
    "    print(f\"output probabilities: {probs[i]}\")\n",
    "    print(f\"actual output [label]: {y}\")\n",
    "    p = probs[i,y]\n",
    "    print(f\"predicted label: {p.item()}\")\n",
    "    \n",
    "    logp = torch.log(p)\n",
    "    nll = -logp\n",
    "    print(f\"log likelihood: {logp}\\tnegative log likelihood: {nll}\")\n",
    "    nlls[i] = nll\n",
    "    print('-'*25)\n",
    "    \n",
    "print('==='*10)\n",
    "print(f\"average nll: {nlls.mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6d4aa97d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0123, 0.0181, 0.0267, 0.0737, 0.0150])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we are interested in the probability that corresponds to the target: probs[input_idx,target_idx]\n",
    "# input_idx: 0,1,2,3,4\n",
    "probs[torch.arange(5), targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d137cc63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.7693)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-probs[torch.arange(5), targets].log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "673ab4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2147483647\n",
    "gen = torch.Generator().manual_seed(seed)\n",
    "W = torch.randn((27,27),generator=gen, requires_grad=True)\n",
    "# forward pass\n",
    "x_enc = F.one_hot(inputs, num_classes=len(tokens)).float()\n",
    "logits = x_enc @ W\n",
    "\n",
    "# these next 2 lines is basically softmax\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "\n",
    "\n",
    "# backward pass\n",
    "\n",
    "W.grad = None # W.grad =0\n",
    "\n",
    "loss = -probs[torch.arange(5), targets].log().mean()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ac82e0ab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 27])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# positive grad: positive influence...adding small h to it will increase the loss\n",
    "W.grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c596328a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization step\n",
    "W.data += -0.1 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f775fc",
   "metadata": {},
   "source": [
    "we see that now the loss has gone from 3.76ish -> 3.74ish, so the model is slightly better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "713fddb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7492127418518066"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = x_enc @ W\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "loss = -probs[torch.arange(5), targets].log().mean()\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88088c10",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Single Layer Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f679c8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('names.txt') as f:\n",
    "    names = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "854b3569",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['.'] + list(string.ascii_lowercase)\n",
    "t_i = {t:i for i,t in enumerate(tokens)}\n",
    "i_t = {i:t for t,i in t_i.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "03aa08cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "targets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5caa860c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in names:\n",
    "    word = ['.'] + list(word) + ['.']\n",
    "    for ch1, ch2 in zip(word,word[1:]):\n",
    "        r = t_i[ch1]\n",
    "        c = t_i[ch2]\n",
    "        inputs.append(r)\n",
    "        targets.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "eae78d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(inputs)\n",
    "targets = torch.tensor(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fb219a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228146"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = inputs.nelement()\n",
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "db7f18ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2147483647\n",
    "gen = torch.Generator().manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ab2452ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = F.one_hot(inputs, num_classes=len(tokens)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "eb7bc3f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 27])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1029abaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 50\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "88b5ed17",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at epoch 10:\t2.711496353149414\n",
      "loss at epoch 20:\t2.579403877258301\n",
      "loss at epoch 30:\t2.533154249191284\n",
      "loss at epoch 40:\t2.510324001312256\n",
      "loss at epoch 50:\t2.497144937515259\n",
      "loss at epoch 60:\t2.488725185394287\n",
      "loss at epoch 70:\t2.4829084873199463\n",
      "loss at epoch 80:\t2.4786534309387207\n",
      "loss at epoch 90:\t2.4754140377044678\n",
      "loss at epoch 100:\t2.4728763103485107\n"
     ]
    }
   ],
   "source": [
    "W = torch.randn((27,27),generator=gen, requires_grad=True)\n",
    "\n",
    "\n",
    "for epoch in range(1,epochs+1):\n",
    "    \n",
    "    # forward pass:\n",
    "    \n",
    "    logits = X @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts/counts.sum(1,keepdims=True)\n",
    "    \n",
    "    loss = -probs[torch.arange(num), targets].log().mean()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"loss at epoch {epoch}:\\t{loss.item()}\")\n",
    "    \n",
    "    # backward pass:\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # optimization:\n",
    "    W.data += -lr * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936250d7",
   "metadata": {},
   "source": [
    "the above W, its W.exp() is equivalent to the N table we obtained via counting.\n",
    "\n",
    "we added fake counts to N, the more we add, the more the probabilities are uniform and smooth.\n",
    "\n",
    "in gradient-descent based learning,\n",
    "\n",
    "we can add regularization which is basically adding a large constant to the loss for more uniform loss\n",
    "___\n",
    "\n",
    "Training loop with regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3e3fc8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at epoch 10:\t2.695918560028076\n",
      "loss at epoch 20:\t2.5837419033050537\n",
      "loss at epoch 30:\t2.5430855751037598\n",
      "loss at epoch 40:\t2.5226268768310547\n",
      "loss at epoch 50:\t2.510843276977539\n",
      "loss at epoch 60:\t2.5034449100494385\n",
      "loss at epoch 70:\t2.4984467029571533\n",
      "loss at epoch 80:\t2.4948818683624268\n",
      "loss at epoch 90:\t2.4922428131103516\n",
      "loss at epoch 100:\t2.4902377128601074\n",
      "loss at epoch 110:\t2.4886841773986816\n",
      "loss at epoch 120:\t2.487460136413574\n",
      "loss at epoch 130:\t2.486481189727783\n",
      "loss at epoch 140:\t2.4856879711151123\n",
      "loss at epoch 150:\t2.4850363731384277\n"
     ]
    }
   ],
   "source": [
    "W = torch.randn((27,27),generator=gen, requires_grad=True)\n",
    "lr = 50\n",
    "epochs = 150\n",
    "\n",
    "for epoch in range(1,epochs+1):\n",
    "    \n",
    "    # forward pass:\n",
    "    logits = X @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts/counts.sum(1,keepdims=True)\n",
    "    \n",
    "    # regularization\n",
    "    regularization_const = torch.pow(W,2).mean()\n",
    "    lmb = 0.01 # more the lambda, more the const, more the impact on loss, more uniform the weights\n",
    "    \n",
    "    # loss with regularization\n",
    "    loss = -probs[torch.arange(num), targets].log().mean()\n",
    "    loss += lmb * regularization_const\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"loss at epoch {epoch}:\\t{loss.item()}\")\n",
    "    \n",
    "    # backward pass:\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # optimization:\n",
    "    W.data += -lr * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766ad94c",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "SAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ca484f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mor.\n",
      "axx.\n",
      "minaymoryles.\n",
      "kondmaisah.\n",
      "anchthizarie.\n",
      "odaren.\n",
      "iaddash.\n",
      "h.\n",
      "jionatien.\n",
      "egwulo.\n",
      "ga.\n",
      "t.\n",
      "a.\n",
      "jayn.\n",
      "ilemannariaenien.\n",
      "ad.\n",
      "f.\n",
      "akiinela.\n",
      "trttanakeroruceyaaxbrima.\n",
      "lamoyonutonadengin.\n",
      "torrederahnn.\n",
      "ellovyllpasskh.\n",
      "a.\n",
      "wai.\n",
      "kole.\n"
     ]
    }
   ],
   "source": [
    "seed = 2147483647\n",
    "gen = torch.Generator().manual_seed(seed)\n",
    "\n",
    "for i in range(25):\n",
    "    \n",
    "    name = ''\n",
    "    idx = 0\n",
    "    while True:\n",
    "        x = F.one_hot(torch.tensor([idx]), num_classes=len(tokens)).float()\n",
    "        logits = x @ W\n",
    "        counts = logits.exp()\n",
    "        p = counts/counts.sum(1, keepdims=True)\n",
    "        \n",
    "        idx = torch.multinomial(p,num_samples=1,replacement=True,generator=gen).item()\n",
    "        name += i_t[idx]\n",
    "        if idx == 0:\n",
    "            break\n",
    "    print(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
